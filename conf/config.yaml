

g2t:
    nhead: 4 # number of attention heads
    head_dim: 128 # dim of each attention head
    weight_decay: 0.0
    n_layers_gat: 2
    vae_dim: 10
    batch_size: 32
    beam_size: 5
    beam_max_len: 50
    enc_lstm_layers: 2
    lr: 2.0e-4
    clip: 1.0 # gradient clipping
    emb_drop: 0.0
    attn_drop: 0.1
    drop: 0.1 # FFN dropout
    length_penalty: 1.0
    save: "g2t_model.pt"
t2g:
    batch_size: 32
    nhid: 512 # number of hidden units
    lr: 5.0e-5
    drop: 0.0
    save: "t2g_model.pt"
    weight_decay: 0.0

main:
    grad_clip: 1.0
    dim_h: 512

    split: 1.1 # proportion of training dataset to consider (does nothing if >=1)
    epoch: 50
    pre_epoch: 10  # warmup epoch if using the warmup
    seed: 654321
    batch_size: 32
    train_file: "train.json"
    dev_file: "dev.json"
    test_file: "test.json"
    display: True
    mode: "sup" #['sup', 'cold_unsup', 'warm_unsup']
