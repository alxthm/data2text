grad_clip: 1.
dim_h: 512
epoch: 50
seed: 654321
batch_size: 32
mode: "sup" # 'sup' or 'unsup'
log_every_n_steps: 50
log_gradients: False
save_checkpoints: False
g2t:
    dim_z: 10
    n_head: 4 # number of attention heads
    head_dim: 128 # dim of each attention head
    weight_decay: 0.0
    n_layers_gat: 2
    beam_size: 5
    beam_max_len: 50
    enc_lstm_layers: 2
    lr: 2.0e-4
    emb_drop: 0.0
    attn_drop: 0.1
    drop: 0.1 # FFN dropout
    length_penalty: 1.0
t2g:
    lr: 5.0e-5
    drop: 0.0
    weight_decay: 0.0
